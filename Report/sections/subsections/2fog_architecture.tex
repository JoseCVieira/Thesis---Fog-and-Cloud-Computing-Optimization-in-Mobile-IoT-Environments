%!TEX encoding = UTF-8 Unicode
\subsection{Fog Computing Architecture}
\label{sec:fog_architecture}
\noindent Fog computing is a great resource to support IoT applications' requirements in mobile environments. Taking into account what has been mentioned in Section \ref{sec:Introduction} and Section \ref{sec:Computingparadigms}, it has the following fundamental characteristics which validate the statement uttered above (refer to Table \ref{computing_paradigms}):
\begin{itemize}
	\item \textbf{Heterogeneity support}. Supports collection and processing of data of different actors acquired through multiple types of network communication, wide diversity applications and services;
	\item \textbf{Geographical distribution}. Uses anything between the cloud and \textit{things} to provide ubiquitous computing, allowing continuity of service in mobile environments;
	\item \textbf{Contextual location awareness, and low latency}. Provides low latency due to the proximity between the IoT devices and the fog nodes. Also, the contextual location allows them to be aware of the cost of communication latency with both other fog nodes and the end devices, allowing the distribution of applications across the network to be organized in a weighted manner;
	\item \textbf{Mobility support}. The exponential growth of mobile devices demands support for mobility techniques, such Locator/ID Separation Protocol (LISP), that decouples the device identity from its location, requiring a distributed directory system;
	\item \textbf{Real-time interactions}. Applications may involve real-time interactions rather than batch processing (e.g., as cloud does);
	\item \textbf{Scalability and agility of federated, fog-node clusters}. Fog is adaptive; may form clusters-of-nodes or cluster-of-clusters to support elastic compute, resource pooling, etc., supporting large-scale applications;
	\item \textbf{Multiple IoT applications}. Fog devices handle multiple IoT applications competing for their limited resources;
	\item \textbf{Virtualization support}. Introduces a software abstraction between the hardware and the OS and application running on the hardware;
	\item \textbf{Interoperability and federation}. Uses cooperation of different providers to support heavy applications such as real-time streaming. Moreover, it supports migration of applications to more suited fog servers depending on the current context;
	\item \textbf{Predominance of wireless access}. Most of the end devices only support wireless communication.
\end{itemize}

Nonetheless, as stated in Section \ref{subsec:Objectives}, fog still has some limitations. In order to tackle those limitations, it is necessary to understand its overall architecture. This includes understanding what are the actors and how they interact, how IoT nodes connect to the fog servers, how clients outsource the allocation and management of resources that they rely upon to these servers, how migration is performed, etc.

\subsubsection{Actors}
\noindent\tab Fig. \ref{fog_architecture} shows the typical fog computing architecture. As stated before, mist computing can be implemented in a layer between the fog servers and the end devices. Moreover, the presence of cloud servers is not imperative, however it is very important for numerous applications.

\begin{figure} [t]
	\centering
	\includegraphics[width=0.9\textwidth]{images/fog_architecture/fog_architecture}
	\caption{Typical architecture of fog computing.}
	\label{fog_architecture}
\end{figure}

\noindent\tab Fog computing layer is composed by fog nodes/servers, that allow the deployment of distributed, latency-aware applications and services. Those nodes can be either physical (e.g., gateways, switches, routers, servers) or virtual (e.g., virtualized switches, virtual machines, cloudlets) components that provide computing resources to end devices. They can be organized in clusters either vertically (to support isolation), horizontally (to support federation), or relative to fog nodesâ€™ latency-distance to the IoT devices \cite{iorga2018fog}. Fog nodes can be accessed through connected devices located at the edge, which provide local computing resources and, when needed, provide network connectivity to centralized services (i.e. cloud). Moreover, fog nodes can operate in a centralized or decentralized manner and can be configured as stand-alone nodes. They are the fundamental components in this three tier architecture (i.e. IoT-fog-cloud), being able to support the six features shown below \cite{iorga2018fog}:
\begin{itemize}
	\item \textbf{Autonomy}. Fog nodes can be autonomous enough to operate independently, making local decisions, at the node or cluster-of-nodes level;
	\item \textbf{Heterogeneity}. Can be deployed in a wide variety of environments;
	\item \textbf{Hierarchical clustering}. The fog network can be organized with different numbers of layers, so that they are able to provide different subsets of service functions while working together as a continuum;
	\item \textbf{Manageability}. They are managed and orchestrated by complex systems that can perform most routine operations automatically;
	\item \textbf{Programmability}. Fog nodes are inherently programmable at multiple levels, by multiple stakeholders such as network operators, domain experts, equipment providers, or end users.
\end{itemize}

\noindent\tab Fog nodes generally are of most value in scenarios where data needs to be collected at the edge and where the data from thousands or even millions of devices is analyzed and acted upon in micro and milliseconds \cite{openfog2017openfog}. In order to being able to support such a large number of requests, especially those engaged in enhanced analytics, fog nodes may implement additional hardware. Accelerators modules (refer to Fig. \ref{fog_architecture}) can be implemented to provide supplementary computational throughput. For instance, hardware accelerators can be performed through Graphics Processing Units (GPUs); they are an optimal choice for applications that support parallelism or for stream processing. Also, fog nodes can opt to make use of Field Programmable Gate Arrays (FPGAs) or even Digital Signal Processors (DSPs) for this propose.\\
\noindent\tab It is worth noting that, once fog nodes can be anything with computational and storage power in the cloud-to-things continuum, the links formed in these architectures (i.e. End device-to-Fog, Fog-to-Fog and Fog-to-Cloud) can be of any type. For instance, end devices can be connected to fog servers by wireless access technologies (e.g., WLAN, WiFi, 3G, 4G, ZigBee, Bluetooth) or wired connection. Moreover, fog nodes can be interconnected by wired or wireless communication technologies and they can be linked into the cloud by core network using fiber transmission with low-latency.\\
\noindent\tab In this architecture, the connected sensors located at the edge, generate data that can adopt two models. First, in a sense-process-actuate model, the information collected is transmitted as data streams, which is acted upon by applications running on fog devices and the resultant commands are sent to actuators. In this model, the raw data collected often does not need to be transferred to the cloud; data can be processed, filtered, or aggregated in fog nodes, producing reduced data sets. The result can then be stored inside fog nodes or actuated upon through the actuators. Second, in a stream-processing model, sensors send equally data streams, where the information mined (from the incoming streams) is stored in data centers for large-scale and long-term analytics. In this case, big data needs to be stored and does not have that much latency constraints. Being fog servers less powerful than the cloud ones, cloud is far more suited for this kind of operations. Yet, fog servers can still shrink data, doing some intermediate processing as in the previous model. This meets the aforementioned statement - although cloud is not essential for the functioning of fog, in some applications it is beneficial or even essential.\\
\noindent\tab The applications deployed by the end users in fog nodes can be seen either as a whole or as a Distributed Data Flow (DDF) programming model, in which the applications are moduled as a collection of modules. DDF is proposed by N. Giang et al. \cite{giang2015developing} for IoT applications that utilizes computing infrastructures across the fog and the cloud, allowing the application flow to be deployed on multiple physical devices rather than one. This can be particular useful so that the less restricted modules in terms of latency can be deployed to the upper fog layers (ideally to the cloud), leaving the fog nodes of the lower layers less overloaded, being able to respond faster to modules within tighter latency bounds. As already mentioned, fog utilizes virtualization mechanisms due to the numerous advantages offered. Hence, hosting an application involves creating a set of VMs or execution containers (e.g., Docker) and assign them to a set of physical or virtual components along the cloud-to-things continuum.

\subsubsection{Orchestration}
\noindent\tab When an end device needs to offload some work to a third party, it needs somehow to know where to outsource the allocation and management of resources. For this propose, the fog architecture also needs a discovery service which concerns in finding the best available fog server, given certain capabilities and requirements. In this context, J. Gedeon et al. \cite{gedeon2017router}, propose a brokering mechanism in which available surrogates (i.e. fog nodes) advertise themselves to the broker. When it receives client requests, considering a number of attributes such as network information, hardware capabilities, and distance, it finds the best available surrogate for the client.\\
%This mechanism can be implemented on standard home routers, and thus, leverages the ubiquity of such devices in urban environments. Multiple brokers are interconnected using Distributed Hash Tables (DHTs) in order to exchange information.\\
%OK
\noindent\tab Finally, fog also needs an orchestration layer in order to monitor the current context and thus be able to make management decisions regarding applications and data placement. In this context, C. Guerrero et al. \cite{guerrero2018influence} state that most of the literature considers a central broker or orchestrator gathering all system information (i.e. monitoring fog devices, clients, cloud and services), leading to poor scalability and high complexity of the orchestration algorithm when the number of elements high (e.g., in smart cities). In order to overcome this bottleneck, O. Skarlat et al. \cite{skarlat2016resource,skarlat2017optimized} consider the concept of fog colonies. Each colony has an orchestrator and an arbitrary number of fog cells, a software component running on fog nodes. These fog cells are responsible for receiving tasks from IoT devices, and depending on the available resources, execute it or transfer it to the orchestrator. This top hierarchic element (inside a colony) will then migrate this task to the cloud or to another fog colony, if the current one is not able to handle it. The authors also state that the orchestrator could be removed using a decentralized approach. However, it would lead to extensive coordination and voting between the involved fog cells. On the other hand, F. Bonomi et al. \cite{bonomi2014fog} propose a distributed orchestration. This is preformed implementing a Foglet software agent in each fog node. It has small footprint yet capable of monitoring machine's health and state. This information is then pushed to the distributed and persistent storage for global processing. The distributed database is also responsible to store the business policies introduced by the administrators. This type of orchestration also implies a scalable messaging bus to carry control messages for service orchestration and resource management. Finally, a distributed policy embedded in every Foglet is proposed using the information stored in the database.\\
%OK
\noindent\tab When the orchestration layer takes the decision to migrate some application/module from one fog node to another, the service continuity is a very important parameter to take into account. The service downtime during the migration may degrade the perceived QoS by the end users. There are several studies in this area. The two approaches that were already proposed are the VMs and the containers. For instance, VM synthesis reduces the image size by splitting images into multiple layers, transferring only the application-specific layer. This includes the static binary programs and runtime memory data \cite{ma2017efficient} which can still involve hundreds of megabytes. Also, live migration was introduced to reduce downtime from the traditional non-live migration where the VM is suspended, transferring all the content from one physical machine to another and resuming only after the process, returning to the same state before suspension. Live migration copies and transfers the VM's processor state without disconnecting denying services to the end devices. Meanwhile the VM's memory state migration can be preformed using two different approaches. On the one hand, the pre-copy memory migration transfers VMâ€™s memory state. As VM keeps running, if a page gets modified (dirtied), it will be re-sent. This keeps going until either a small, Writable Working Set (WWS) has been identified, or a given number of iterations is reached. Then the VM is suspended and sent along with the remaining dirtied pages to the target machine. On the other hand, post-copy creates and send a snapshot of the VM state from de source physical machine to the destination, where it is started. Meanwhile, during the transference, the VM is still running in the source machine. Upon copy completion, the memory state that was keep changing is copied on demand (page-faults) from the source to the destination machine, reducing the downtime service \cite{hines2009post}.\\
%OK
\noindent\tab Despite these advances, the use of VM in the migration process can be impractical in fog environments due to large size transfer. Container raises as a lighter virtualization technique. It allows developers to package up an application with all of the parts it needs (e.g., libraries and other dependencies). These applications share the OS and some libraries and/or binaries, allowing container's size to be much smaller. This eases both the migration and hosting applications (e.g., more applications in a single machine and does not require to restart the OS upon migration) \cite{saurez2016incremental}.\\
%Ã© necessÃ¡rio falar em docker container??
%OK
\noindent\tab Moreover, there two different approaches that can be used in the migration mechanism. On the one hand, the reactive approach, as the name suggests, only performs migration when it is needed. When user and fog node become out of range, migration is performed to ensure QoS in mobile environments. I. Farris et al. \cite{farris2017optimizing} argue that downtime is not the only degrading factor of service continuity, but also the overall migration which impacts user's Quality of Experience (QoE). On the other hand, the proactive approach deploys replicas of the user service in neighboring fog nodes. Also, periodically the state in these nodes is updated. The main goal is to reduce drastically the migration time. However, this brings new costs at different levels such as computation, memory, and networking.\\
%For this propose, E. Saurez et al. \cite{saurez2016incremental} propose foglets, a programming model that facilitates distributed programming across fog nodes. It is implemented through container-based visualization. Foglets not only provides APIs for application development as DDFs, including the primitives for communication between the application components, but also embodies algorithms for the discovery and incremental deployment of resources commensurate with the application needs. Moreover it provides mechanisms for QoS-sensitive and workload sensitive migration of application components due to end devices mobility and application dynamism. Specifically, there are four entities in the foglets runtime system: the discovery server, the docker registry server, the entry point daemon, and the worker process. The discovery server is a partitioned name server that maintains a list of fog nodes available. Docker registry server is a server that contains the binaries for the applications that have been launched on the foglets infrastructure. The entry point daemon executes directly on top of the host OS in the fog node, awaits requests and periodically sends ``I am alive'' message to the discovery server. Finally the worker process carries out the functionality contained in a particular application component assigned to it.\\
\noindent\tab Fog servers can provide reduced latencies and help in avoiding/reducing traffic congestion in the network core. However, this comes at a price: more complex and sophisticated resource management mechanisms are needed. This raises new challenges to be overcome such as dynamically deciding when, and where (device/fog/cloud) to carry out processing of requests to meet their QoS requirements. Furthermore, in mobile environments such mechanisms must incorporate mobility (i.e. location) of data sources, sinks and fog servers in the resource management and allocation process policies to promote and take advantage of proximity between fog and users.