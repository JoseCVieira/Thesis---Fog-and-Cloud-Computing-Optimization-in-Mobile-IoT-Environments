%!TEX encoding = UTF-8 Unicode
\subsection{Migration optimization in mobile fog environments}
\label{sec:Migration}
When an IoT device needs to offload some heavy application to a third party, it will be connected to the nearest server, securing an one-hop away fog server to ensure the shortest network delay. However, as their physical distance increases either by device or server movement, their network distance (i.e. the number of hops) will also increase. Hence, both latency and bandwidth usage by the intermediate links will increase, resulting in poor connectivity. This away, in such dynamic environments the decision-making of where to send the VMs, in order to overcome these limitations, is a major concern. Moreover, even if both clients and servers are static, the end-to-end latency may increase due to unexpected crowds of mobile clients seeking to connect or making requests to the same fog server simultaneously.\\
\noindent\tab In Section \ref{sec:fog_architecture} was verified that applications can be offloaded as a whole, or as a set of modules that may have different constraints. Regardless of their type, whenever it is justified the system needs to be readjusted. This is performed through the exchange of VMs (that contain the applications or modules) between fog nodes. For this reason, it is necessary to answer the following two questions: \textit{When is this exchange justified? And what is the best placement for those applications and/or modules?}\\

%%Previous reactive load balancing algorithms migrate VMs upon the occurrence of load imbalance, while previous proactive load balancing algorithms predict PM overload to conduct VM migration
% another works were performd using MDP-based approaches (ex. Distributed Autonomous Virtual Resource Management in Datacenters Using Finite-Markov Decision Process)

\subsubsection{Latency-aware with mobile end-devices}
\noindent\tab In the context of mobile end devices, B Ottenwälder et al. \cite{ottenwalder2013migcep} state that as sensors and consumers are mobile, the latency (between the access point of a mobile sensor and the fog node) and bandwidth usage is expected to change over time, so it is necessary to constantly adapt the placement through migrations to new fog nodes. However, each migration comes with a cost; consequence of the local state that also needs to be migrated. Thus, frequent migration would significantly decrease the system performance. To overcome this limitation, they propose a placement and migration method for providers of infrastructures that incorporate cloud and fog resources to support operator migrations in mobile complex event processing (MCEP) systems. Their method plans the migration ahead of time through knowledge of the MCEP system and predicted mobility patterns towards ensuring application-defined end-to-end latency restrictions and reducing the network utilization. These predicted mobility patterns were captured using three different methods: uncertain locations from the \textit{dead reckoning} approach (linear), certain locations that could stem from a \textit{navigation} system (navi), and \textit{learned} transitions between leaf broker (learned). This method, allows a minimization of migration costs by selecting migration targets that ensure a low expected network utilization for a sufficiently long time. Moreover, they present how the application knowledge of the CEP system can be used to improve current live migration techniques for VMs to reduce the required bandwidth during the migration (i.e. unnecessary events are not migrated).\\
\noindent\tab A different approach was adopted in the work of R Urgaonkar et al. \cite{urgaonkar2015dynamic}. Similarly to the previous work, their aim is to provide an optimal decision with regard to: where to migrate a current service as the user location changes. However, they argue that because of the uncertainty in user mobility and request patterns, it is challenging to make the decision in an optimal manner. Also, in this work is argued that methods that depend on mobility patterns have several drawback, namely: (1) it requires extensive knowledge of the statistics of the user mobility and request arrival processes that can be impractical to obtain in a dynamic network, (2) even when this is known, the resulting problem can be computationally challenging to solve, and (3) any change in the statistics would make the previous solution suboptimal and would require recomputing the optimal solution. Thus, they propose a new model, inspired by the technique of Lyapunov optimization, that overcomes these drawbacks (i.e. does not require any knowledge of the transition probabilities). The overall problem of dynamic service migration and workload scheduling to optimize system cost while providing end user performance guarantees is formulated as a sequential decision making problem in the framework of markov decision problems (MDPs). However, they have developed a new approach for solving a class of constrained MDPs that possess a decoupling property. When this property holds, their approach enables the design of simple online control algorithms that do not require any knowledge of the underlying statistics of the MDPs.\\
%This technique was applied to both dynamic service migration and workload scheduling and compared to three other policies: never migrate policy, always migrate policy and myopic policy (recalculate the system parameters in time slots). \\
\noindent\tab W Zhang et al. \cite{zhang2016segue} state that previous studies have proposed a static distance-based MDP for optimizing migration decisions. However, these models fail to consider dynamic network and server states in migration decisions, assuming that all the important variables are known. Moreover, they also point out another unaddressed problem which lies in the recalculation time interval of the method. Since running MPD is a heavy computing task, a short recalculation interval introduces a considerable overhead to the server. On the other hand, a long recalculation interval may translate into lazy migration, meaning that resulting in periods of transgression of QoS guarantees. In order to overcome those issues, the authors propose SEGUE. This model achieves optimal migration decisions by providing a long-term optimal QoS to mobile users in the presence of link quality and server load variation. Additionally, SEGUE adopts a QoS aware scheme to activate the MDP model. In other words, it only activates the MDP model when QoS violation is predicted. Thus, it avoids unnecessary migration costs and bypass any possible QoS violations while keeping a reasonable low overhead in the servers.\\
\noindent\tab The work performed by Wuyang Zhang et al. \cite{Zhang2017} use as case study the Massively Multiplayer Online Gamse (MMOGs) with Virtual Reality (VR) technologies, VR-MMOGs. They present the main challages of VR-MMOGs, namely: stringent latency, high bandwidth, and large scale requirements. This work shows one problem that remains unsolved: how to distribute the work among the user device, the edge clouds, and the center cloud to meet all three requirements especially when users are mobile. Their approach was to place local view change updates on edge clouds for immediate responses, frame rendering on edge clouds for high bandwidth, and global game state updates on the center cloud for user scalability. In this kind of games, the users need to move, so in order to keep a low latency communication, they also ropose an efficient service placement algorithm based on MDP. This method takes into account the presence of dynamic network states and server workload states, and user mobility. To ensure feasibility of this method, they come up with an approach that reduces the algorithm complexity in both storage and execution time. Nonetheless, unlike many of the service migration solutions which assumes an ignorable service transition time, they point out that it is impossible to migrate an edge service from one edge to another instantly given the size of a VR game world. Therefore, they propose a mechanism to ensure a new edge cloud is activated when a player connects to the new one.\\
\noindent\tab S Abdelwahab et al. \cite{abdelwahab2018clones} argue that IoT devices communicate a large number of messages with many devices. Thus, devices with low computing and storage capacities will became another source of latency for large-scale distributed applications. Their experiments show that brokering the messages through a one-hop away broker may reduce significantly the end-to-end latency. Therefore, if devices are cloned in a one-hop away cloudlet, their clones can provide message brokering service, allowing both a communication with minimal latency between devices and to offload intensive computation into rich memory and processing nodes that host the clones. Nonetheless, communicating through a one-hop away clone may still experience long end-to-end latency when the broker service relays messages to distant devices. Hence, they propose FogMQ, a self-deploying brokering clones that discover hosting platforms and autonomously migrate between them according to the measured end-to-end latency. This method does not need a central monitoring and control unit. FogMQ servers expose tomography functionalities that enables clones to take migration decisions without complete knowledge about the hosting platform. It allows to stabilize clones deployment and achieve a near minimum latency given an existing infrastructure limits.\\
\noindent\tab The study performed by X Sun et al. \cite{sun2016primal} presents, similarly to the previous ones, a case scenario where end devices are mobile. To preform this work they use a cloudlet network architecture to bring the computing resources from the centralized cloud to the edge. They present the PRofIt Maximization Avatar pLacement (PRIMAL) strategy. PRIMAL maximizes the trade-off between the migration gain (i.e. the end-to-end delay reduction) and the migration cost (i.e. the migration overheads), selectively migrating the avatars (an application clone located in a cloudlet) to their optimal locations. %Does not address where this strategy is calculated and the time interval (verificar melhor).

\subsubsection{Latency-aware with mobile fog servers}
%The concept of mobile fog computing is similar to fog computing, in which both IoT and fog nodes are mobile components. Those are connected wirelessly (e.g., via WiFi or Bluetooth). The challenge with implementing fog computing in mobile environments lies in the underlying complexity of data placement management and decision-making to ensure the QoS to all users (i.e. ensure that all latency constraints of users' applications are met).\\
%\noindent\tab Although several studies were already done in order to provide mobile support for IoT devices, the purpose of this study is to support mobile fog computing, once fog nodes can be anything in the path that connects things to the cloud. This distributed middle tier, in the 3-tier architecture (things-fog-cloud), can use as fog nodes any physical device that has facilities or infrastructures that can provide resources and visualization capabilities. This, may include movable fog nodes, such as cars, buses, unmanned aerial vehicles (UAVs), etc. The importance of mobile fog nodes cannot be overlooked, once they may represent a way to offload fixed cloudlet tasks and thus improve fog features. In this field there are already some early efforts.\\
D Ye et al. \cite{ye2016scalable} leverage the characteristics of buses and propose a scalable fog computing paradigm with servicing offloading in bus networks. Knowing that buses have fixed mobility trajectories and strong periodicity, they consider a fog computing paradigm with service offloading in bus networks which is composed by two parts: roadside cloudlets and bus fog servers. The roadside cloudlet consists of three components: dedicated local servers, location-based service (LBS) providers, access points (APs). The dedicated local servers virtualize physical resource and act as a potential cloud computing site. The LBS providers offer the real time location of each bus in bus networks. The APs act as gateways for mobile users and bus fog servers within the communication coverage to access the roadside cloudlet. When users need to offload some computationally intensive and delay sensitive tasks, they access APs and use the computing service of the roadside cloudlet. However, as cloudlets have limited computational and storage resources, they may became overloaded. The bus fog server is a virtualized computing system on bus, which is similar to a light-weight cloudlet server. Hence, those buses not only provide fog computing services for the mobile users on bus, but also are motivated to accomplish the computation tasks offloaded by roadside cloudlets. This allocation strategy is accomplished using genetic algorithm (GA), where the objective is to minimize the cost that roadside cloudlets spend to offload their computation tasks. Meanwhile, the user experience of mobile users are maintained. Although this work refers to mobile users, its meaning is not literal, being supported only the mobility of fog servers. In their problem formulation there are two types of mobile users. On one hand there are mobile users that already have offloaded their computing tasks to the roadside cloudlets (i.e. representing the workload of the cloudlets). On the other hand there are several mobile users inside the bus that have also offloadded their tasks (i.e. representing the workload of bus fog servers).\\
MM Lopes et al. \cite{lopes2017myifogsim} discuss resource allocation in fog computing in the face of users’ mobility, where mobility is achieved through migration of virtual machines between cloudlets. They present a new migration technique composed of two modules: migration policy which defines when the user VM should be migrated, considering aspects such as the user's speed, direction and geographical position and migration strategy, the destination cloudlet, and how the migration is performed. This work had the objective of study the impact of different migration strategies in the latency with users’ mobility.

\subsubsection{Energy-aware}
One of the main objectives of fog computing is to improve the service quality of users at the edge of the network. This includes both guarantee compliance with their latency constants and minimizing their energy consumption. However, in order to achieve this objective, the placement of applications and their modules has often to be moved between different entities that compose the 3-tier architecture (things-fog-cloud) what evolves costs. For instance, it is needed exchange control messages, communicate between modules placed at different nodes, change the module placement, etc. Thus, energy-aware must be an important factor to be taken into account in the decision making algorithm of \textit{when} and \textit{where} to offload work to another entity.\\
\noindent\tab In order to accomplish the aforementioned, QoS, objective, Z Chang et al. \cite{chang2017energy} investigates the energy efficient computation offloading scheme in a multi-user fog computing system. Based on their energy consumption and delay constraint, users need to make the decision on whether to offload the tasks to the fog node nearby. Using queuing theory, they bring a thorough study on the energy consumption and execution delay of the offloading process. Two queuing models are applied respectively to model the execution processes at the mobile device (MD) and fog node. Based on the theoretical analysis, they present an energy consumption optimization problem with explicit consideration of delay performance. They propose an Alternating Direction Method of Multipliers (ADMM)-based to address the formulated problem in a distributed manner. Similarly, TQ Dinh et al. \cite{dinh2017offloading} propose an optimization framework of offloading from a single mobile device (MD) to multiple edge devices. The objective of this work is to minimize both total tasks’ execution latency and the mobile device’s energy consumption by jointly optimizing the task allocation decision and the mobile device’s central process unit (CPU) frequency by using Dynamic Voltage Frequency Scalling (DVFS). They consider two case studies: fixed CPU frequency and elastic CPU frequency. The formulation of these problems are NP-hard, so they propose a linear relaxation-based approach and a semidefinite relaxation (SDR)-based approach for the fixed CPU frequency case, and an exhaustive searchbased approach and an SDR-based approach for the elastic CPU frequency case.\\
\noindent\tab As before mentioned, for fog infrastructures providers, besides guarantee the QoS to the users, they also want to minimize the energy consumption of fog nodes in order to increase their profit. In this context, Y Xiao et al. \cite{xiao2017qoe} investigate two performance metrics for fog computing networks: mobile users’ QoE and fog nodes’ power efficiency. In their scheme fog nodes can process or offload to other fog nodes part of the workload that was initially sent to the cloud. Fog nodes decide to either offload the workload to neighbors or locally process it, under a given power constraint. A distributed optimization algorithm based on distributed Alternating Direction Method of Multipliers (ADMM) via variable splitting algorithm is proposed to achieve the optimal workload allocation solution that maximizes users’ QoE under the given power efficiency. Similarly, R Deng et al. \cite{deng2016optimal} focused on investigating power consumption and network delay tradeoff in cloud-fog services. For this propose they develop an approximate approach to solve the primal problem through decomposition, and formulate three subproblems of three corresponding subsystems. These subproblems can be, respectively, solved via existing optimization techniques. Still in this context, A Kattepur et al. \cite{kattepur2016resource} investigate the problem of computation offloading in fog computing. They present an energy model and communication costs with respect to computational offloading and formulate an optimal deployment strategy when dealing with distributed computation, while keeping energy and latency constraints in mind. The formulations are solved in Scilab using the Karmarkar linear optimization solver. They evaluate their approach in a sense-process-actuate model using a network of mobile robotic sensor-actuators developed in ROS/Gazebo.\\
\noindent\tab Different from the previous works, C Anglano et al. \cite{anglano2018profit} present the Online Profit Maximization (OPM) algorithm. It is an approximation algorithm that aims increasing the profit, by reducing the overall consumption of the infrastructure without a priori knowledge, and yet guarantee the QoS to its users. Their study considers mobile environments, where the end devices that are generating that are moving. Similarly, Y Nan et al. \cite{nan2017adaptive} describe an online algorithm, Lyapunov Optimization on Time and Energy Cost (LOTEC), based on the technique of Lyapunov optimization. LOTEC is a quantified near optimal solution and is able to make control decision on application offloading by adjusting the two-way tradeoff between average response time and average cost. This decision-making distributes the incoming applications to the corresponding tiers without a priori knowledge of the status of users and system.
%[172] 2016 **Vehicular fog computing: A viewpoint of vehicles as the infrastructures.**
%Xueshi Hou et al. present the idea of utilizing vehicles as the infrastructures for communication and computation, named vehicular fog computing (VFC), which is an architecture that utilizes a collaborative multitude of end-user clients or near-user edge devices to carry out communication and computation, based on better utilization of individual communication and computational resources of each vehicle. They discussed four types of scenarios of moving and parked vehicles or congested traffic. Also, they point out the advantages against vehicular cloud computing (VCC) and the advantages in scenarios like of emergency operations for natural disaster and terrorist attack.\\

\subsubsection{Bandwidth-aware}
xxx

\subsubsection{Cost-aware}
xxx

\subsubsection{QoS-aware}
(acho que será eliminado)

\subsubsection{Workload-aware}
(acho que será eliminado - tinhamos falado que não se fala explissitamente, mas que o algoritmo será responsável por conseguir atingir este objetivo)

\subsubsection{Handover}
\noindent\tab X Sun et al. \cite{sun2017avaptive} shows an architecture where each User Equipment (UE) has its own Avatar (a private computing and storage resources for the UE) which is deployed to a cloudlet, being the communication characterized by low end-to-end (E2E) latency. When UEs roam away, in order to maintain the end-to-end latency, their Avatars should be handed off among cloudlets accordingly. However, moving such amount of data (the Avatar’s virtual disk) during the handoff time may both incur unbearable migration time and network congestion. In order to overcome those limitations, they propose LatEncy Aware Replica placemeNt (LEARN) algorithm to place a number of replicas of each Avatar’s virtual disk into suitable cloudlets. Meanwhile, by considering the capacity limitation of each cloudlet, they propose the LatEncy aware Avatar hanDoff (LEAD) algorithm to place
UEs’ Avatars among the cloudlets such that the average E2E delay is minimized.\\ %nao sei se devia estar aqui!
\cite{bao2017follow}
The authors observe that traditional mobile network handover mechanisms cannot handle the demands of fog computation resources and the low-latency requirements of mobile IoT applications. The authors propose Follow Me Fog framework to guarantee service continuity and reduce latency during handovers. The key idea proposed is to continuously monitor the received signal strength of the fog nodes at the mobile IoT device, and to trigger pre-migration of computation jobs before disconnecting the IoT device from the existing fog node.\\
\cite{ma2017efficient}
Present a novel service handoff system which seamlessly migrates offloading services to the nearest edge server, while the mobile client is moving. Service handoff is achieved via container migration. They have identified an important performance problem during Docker container migration, proposing a migration method which leverages the layered storage system to reduce file system synchronization overhead, without dependence on the distributed file system.\\
\cite{sun2017emm}
Develop a novel user-centric energy-aware mobility management (EMM) scheme, in order to optimize the delay, under energy consumption constraint of the user. Based on Lyapunov optimization and multi-armed bandit theories, EMM works in an online fashion. Theoretical analysis explicitly takes radio handover and computation migration cost into consideration and proves a bounded deviation on both the delay performance and energy consumption compared with the oracle solution with exact and complete future system information. The proposed algorithm also effectively handles the scenario in which candidate BSs randomly switch ON/OFF during the offloading process of a task.\\
\cite{bi2018mobility}
Study of mobility support issue in fog computing for guaranteeing service continuity. Propose a novel SDN enabled architecture that is able to facilitate mobility management in fog computing by decoupling mobility management and data forwarding functions. Design an efficient handover scheme by migrating mobility management and route optimization logic to the SDN controller. By employing link layer information, the SDN controller can pre-compute the optimal path by estimating the performance gain of each path.\\
\cite{farris2017optimizing}
To guarantee the strict latency requirements, new solutions are required to cope with the user mobility in a distributed edge cloud environment. The use of proactive replication mechanism seems promising to avoid QoE degradation during service migration between different edge nodes. However, accounting for the limited resources of edge micro data-centers, appropriate optimization solutions must be developed to reduce the cost of service deployment, while guaranteeing the desired QoE. In this paper, Ivan Farris et al., by leveraging on prediction schemes of user mobility patterns, have proposed two linear optimization solutions for replication-based service migration in cellular 5G networks: the min-RM approach aims at minimizing the QoE degradation during user handover; min-NSR approach favors the reduction of service replication cost. Simulation results proved the efficiency of each solution in achieving its design goal and provides useful information for network and service orchestrators in next-generation 5G cloud-based networks.
%\noindent\tab Fog computing will be crucial in a diversity of scenarios. For
%instance, heterogeneous sensory nodes (e.g., sensors, controllers, actuators)
%on a self-driving vehicle, are estimated to generate about 1GB data per second
%\cite{angelica2013google}. As the number of features grow, the data deluge
%grows out of control. Moreover, these types of systems, where people's lives
%depends on it, are hard real-time what means that it is absolutely imperative
%that all deadlines are met. Offloading tasks to fog nodes will be the best
%solution, once a big effort in mobility support has been done through the
%migration of VMs using cloudlets \cite{lopes2017myifogsim}. Also, in this
%context, Puliafito et al. address three types of applications where fog is
%required, namely, citizen's healthcare, drones for smart urban surveillance and
%tourists as time travellers \cite{puliafito2017fog}, addressing the needs of
%low latency and mobility support.\\